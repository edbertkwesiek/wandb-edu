{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "27Y_-TJ1i3kz"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install set-env-colab-kaggle-dotenv -q\n",
        "!pip install weave -U -q\n",
        "!pip install litellm -U -q\n",
        "!pip install openai\n",
        "!pip install text-formatter\n",
        "\n",
        "#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"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.colab"
      ],
      "metadata": {
        "id": "MxK8NUnJMzXk"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import weave\n",
        "import litellm\n",
        "completion = litellm.completion\n",
        "from google.colab import userdata\n",
        "userdata.get('OPENAI_API_KEY')\n",
        "from openai import OpenAI\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "\n",
        "\n",
        "#from openai import OpenAI\n",
        "#client = OpenAI()\n",
        "\n",
        "#response = client.responses.create(\n",
        " #   model=\"gpt-4.1\",\n",
        "  #  input=\"Write a one-sentence bedtime story about a unicorn.\"\n",
        "#)\n",
        "\n",
        "#print(response.output_text)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "60gXPBDYjwLO"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "from set_env import set_env\n",
        "set_env(\"ANTHROPIC_API_KEY\")\n",
        "set_env(\"WANDB_API_KEY\")\n",
        "set_env(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "wIESNZCtL9p5"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the necessary Google client libraries\n",
        "!pip install -q google-api-python-client google-auth-httplib2 google-auth-oauthlib\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import mimetypes # To guess the file type for upload\n",
        "import tempfile  # For creating temporary files in Colab\n",
        "import io        # For handling byte streams\n",
        "\n",
        "from google.colab import files as colab_files # For local computer to Colab upload\n",
        "from google.colab import auth as colab_auth\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.http import MediaFileUpload, MediaIoBaseDownload # Added MediaIoBaseDownload\n",
        "from googleapiclient.errors import HttpError"
      ],
      "metadata": {
        "id": "LtVk74mxzVIA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_or_create_drive_folder_id(service, folder_path_str, parent_id='root'):\n",
        "    \"\"\"\n",
        "    Finds the ID of a Drive folder path, creating folders if they don't exist.\n",
        "    (Same as the previous example, slightly condensed here for brevity)\n",
        "    \"\"\"\n",
        "    current_parent_id = parent_id\n",
        "    folder_names = [name for name in folder_path_str.strip('/').split('/') if name]\n",
        "    if not folder_names: return current_parent_id\n",
        "    print(f\"Resolving Drive folder path: '{'/'.join(folder_names)}' from parent ID: '{current_parent_id}'\")\n",
        "    for folder_name in folder_names:\n",
        "        # print(f\"  Searching for folder '{folder_name}' within parent ID '{current_parent_id}'...\")\n",
        "        try:\n",
        "            query = (f\"mimeType='application/vnd.google-apps.folder' \"\n",
        "                     f\"and name='{folder_name}' \"\n",
        "                     f\"and '{current_parent_id}' in parents \"\n",
        "                     f\"and trashed=false\")\n",
        "            response = service.files().list(q=query, spaces='drive', fields='files(id, name)', pageSize=1).execute()\n",
        "            found_folders = response.get('files', [])\n",
        "            if found_folders:\n",
        "                folder_id = found_folders[0].get('id')\n",
        "                # print(f\"  Found existing folder '{folder_name}' with ID: {folder_id}\")\n",
        "                current_parent_id = folder_id\n",
        "            else:\n",
        "                # print(f\"  Folder '{folder_name}' not found. Creating it...\")\n",
        "                folder_metadata = {'name': folder_name, 'mimeType': 'application/vnd.google-apps.folder', 'parents': [current_parent_id]}\n",
        "                created_folder = service.files().create(body=folder_metadata, fields='id').execute()\n",
        "                folder_id = created_folder.get('id')\n",
        "                # print(f\"  Successfully created folder '{folder_name}' with ID: {folder_id}\")\n",
        "                current_parent_id = folder_id\n",
        "        except HttpError as error:\n",
        "            print(f\"  ERROR: API error during folder resolution for '{folder_name}': {error}\")\n",
        "            return None\n",
        "    print(f\"Target Drive folder resolved to ID: {current_parent_id}\")\n",
        "    return current_parent_id"
      ],
      "metadata": {
        "id": "Exr7aQfup2ly"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def upload_local_to_drive_and_read(\n",
        "    drive_folder_path,\n",
        "    drive_filename_override=None,\n",
        "    overwrite_in_drive=False,\n",
        "    read_mode='r',\n",
        "    read_encoding='utf-8'\n",
        "):\n",
        "    \"\"\"\n",
        "    Prompts user to upload a file from their local computer to Colab,\n",
        "    then uploads that file to Google Drive, and finally reads and outputs\n",
        "    the content of the file from Google Drive.\n",
        "\n",
        "    Args:\n",
        "        drive_folder_path (str): Destination path in Google Drive, relative to 'My Drive'\n",
        "                                 (e.g., 'Colab Uploads/Reports' or ''). Folders will be\n",
        "                                 created if they don't exist. Use \"\" or \"/\" for root 'My Drive'.\n",
        "        drive_filename_override (str, optional): Name for the file in Google Drive.\n",
        "                                                 If None, uses the original local filename.\n",
        "        overwrite_in_drive (bool, optional): If True and a file with the same name exists\n",
        "                                             in the target Drive folder, it will be overwritten.\n",
        "                                             Defaults to False.\n",
        "        read_mode (str, optional): Mode for reading the file from Drive ('r' for text,\n",
        "                                   'rb' for binary). Defaults to 'r'.\n",
        "        read_encoding (str, optional): Encoding for reading text files. Defaults to 'utf-8'.\n",
        "                                       Ignored if read_mode is 'rb'.\n",
        "\n",
        "    Returns:\n",
        "        str or bytes: The content of the file read from Google Drive if successful.\n",
        "                      Returns a string (str) if read_mode is 'r'.\n",
        "                      Returns bytes if read_mode is 'rb'.\n",
        "        None: If any part of the process fails.\n",
        "    \"\"\"\n",
        "    print(\"--- Initiating Local Upload to Drive and Read Process ---\")\n",
        "    local_colab_temp_path = None  # To store path of temp file in Colab\n",
        "    drive_service = None\n",
        "    uploaded_drive_file_id = None\n",
        "\n",
        "    try:\n",
        "        # --- 1. Upload from Local Computer to Colab Environment ---\n",
        "        print(\"\\nStep 1: Uploading file from your local computer to Colab...\")\n",
        "        uploaded_from_local = colab_files.upload()\n",
        "\n",
        "        if not uploaded_from_local:\n",
        "            print(\"  No file selected or upload cancelled by user.\")\n",
        "            return None\n",
        "\n",
        "        # Get the filename and content (bytes) from the uploaded dictionary\n",
        "        original_local_filename = list(uploaded_from_local.keys())[0]\n",
        "        file_content_bytes = uploaded_from_local[original_local_filename]\n",
        "        print(f\"  Successfully uploaded '{original_local_filename}' ({len(file_content_bytes)} bytes) to Colab environment.\")\n",
        "\n",
        "        # --- 2. Save to a Temporary File in Colab ---\n",
        "        # MediaFileUpload needs a file path, so save bytes to a temp file\n",
        "        # We add the original filename as a suffix to help with mimetype guessing and debugging\n",
        "        with tempfile.NamedTemporaryFile(delete=False, mode='wb', suffix=f\"_{original_local_filename}\") as tmp:\n",
        "            tmp.write(file_content_bytes)\n",
        "            local_colab_temp_path = tmp.name\n",
        "        print(f\"  Saved uploaded content to temporary Colab file: '{local_colab_temp_path}'\")\n",
        "\n",
        "        # --- 3. Authenticate and Prepare for Google Drive API ---\n",
        "        print(\"\\nStep 2: Authenticating with Google Drive API...\")\n",
        "        try:\n",
        "            colab_auth.authenticate_user()\n",
        "            drive_service = build('drive', 'v3')\n",
        "            print(\"  Google Drive API authentication successful and service created.\")\n",
        "        except Exception as e:\n",
        "            print(f\"  ERROR: Failed to authenticate or build Drive service: {e}\")\n",
        "            return None # Exit if authentication fails\n",
        "\n",
        "        # --- 4. Resolve Target Folder and Upload to Google Drive ---\n",
        "        print(\"\\nStep 3: Uploading file from Colab to Google Drive...\")\n",
        "        target_drive_filename = drive_filename_override if drive_filename_override else original_local_filename\n",
        "        print(f\"  Target Drive path: '{drive_folder_path if drive_folder_path else 'My Drive'}'\")\n",
        "        print(f\"  Target Drive filename: '{target_drive_filename}'\")\n",
        "\n",
        "        target_folder_id = _get_or_create_drive_folder_id(drive_service, drive_folder_path, parent_id='root')\n",
        "        if target_folder_id is None:\n",
        "            print(\"  ERROR: Failed to resolve or create the target Google Drive folder.\")\n",
        "            return None\n",
        "\n",
        "        # Check for existing file in Drive and handle overwrite\n",
        "        existing_drive_file_id = None\n",
        "        query = (f\"name='{target_drive_filename}' \"\n",
        "                 f\"and '{target_folder_id}' in parents \"\n",
        "                 f\"and mimeType != 'application/vnd.google-apps.folder' \"\n",
        "                 f\"and trashed=false\")\n",
        "        response = drive_service.files().list(q=query, spaces='drive', fields='files(id, name)').execute()\n",
        "        existing_files = response.get('files', [])\n",
        "\n",
        "        if existing_files:\n",
        "            existing_drive_file_id = existing_files[0]['id']\n",
        "            print(f\"  Found existing file '{target_drive_filename}' in Drive (ID: {existing_drive_file_id}).\")\n",
        "            if not overwrite_in_drive:\n",
        "                print(\"  Overwrite is False. Using existing file in Drive for reading.\")\n",
        "                uploaded_drive_file_id = existing_drive_file_id # Use existing file ID\n",
        "            else:\n",
        "                print(\"  Overwrite is True. Deleting existing file in Drive...\")\n",
        "                drive_service.files().delete(fileId=existing_drive_file_id).execute()\n",
        "                print(f\"  Successfully deleted existing Drive file ID: {existing_drive_file_id}\")\n",
        "                existing_drive_file_id = None # Clear it\n",
        "        else:\n",
        "             print(f\"  No existing file named '{target_drive_filename}' found in the target Drive folder.\")\n",
        "\n",
        "\n",
        "        if not uploaded_drive_file_id: # If not using an existing file, proceed with upload\n",
        "            mimetype, _ = mimetypes.guess_type(local_colab_temp_path) # Guess from temp file name\n",
        "            if mimetype is None:\n",
        "                mimetype = 'application/octet-stream'\n",
        "\n",
        "            file_metadata = {'name': target_drive_filename, 'parents': [target_folder_id]}\n",
        "            media = MediaFileUpload(local_colab_temp_path, mimetype=mimetype, resumable=True)\n",
        "\n",
        "            print(f\"  Uploading '{target_drive_filename}' (MIME: {mimetype})...\")\n",
        "            uploaded_file_details = drive_service.files().create(\n",
        "                body=file_metadata,\n",
        "                media_body=media,\n",
        "                fields='id, name, webViewLink'\n",
        "            ).execute()\n",
        "            uploaded_drive_file_id = uploaded_file_details.get('id')\n",
        "            print(f\"  SUCCESS: File uploaded to Google Drive.\")\n",
        "            print(f\"    Drive File Name: {uploaded_file_details.get('name')}\")\n",
        "            print(f\"    Drive File ID: {uploaded_drive_file_id}\")\n",
        "            print(f\"    Drive File Link: {uploaded_file_details.get('webViewLink')}\")\n",
        "\n",
        "        if not uploaded_drive_file_id:\n",
        "            print(\"  ERROR: Failed to obtain a Google Drive file ID for reading.\")\n",
        "            return None\n",
        "\n",
        "        # --- 5. Read File Content from Google Drive ---\n",
        "        print(f\"\\nStep 4: Reading file content from Google Drive (ID: {uploaded_drive_file_id})...\")\n",
        "        request = drive_service.files().get_media(fileId=uploaded_drive_file_id)\n",
        "        fh = io.BytesIO() # Use BytesIO to accumulate downloaded bytes\n",
        "        downloader = MediaIoBaseDownload(fh, request)\n",
        "        done = False\n",
        "        while done is False:\n",
        "            status, done = downloader.next_chunk()\n",
        "            if status:\n",
        "                 print(f\"  Download progress: {int(status.progress() * 100)}%.\")\n",
        "        print(\"  File content downloaded from Drive.\")\n",
        "\n",
        "        downloaded_bytes = fh.getvalue()\n",
        "\n",
        "        if read_mode == 'r':\n",
        "            try:\n",
        "                content_from_drive = downloaded_bytes.decode(read_encoding)\n",
        "                print(f\"  Successfully decoded {len(content_from_drive)} characters (encoding: {read_encoding}).\")\n",
        "                return content_from_drive\n",
        "            except UnicodeDecodeError as e:\n",
        "                print(f\"  ERROR: UnicodeDecodeError while reading from Drive with encoding '{read_encoding}'. {e}\")\n",
        "                print(f\"  Try a different encoding or use read_mode='rb'. Raw bytes preview: {downloaded_bytes[:100]}\")\n",
        "                return None # Or return raw bytes if preferred on decode error\n",
        "        elif read_mode == 'rb':\n",
        "            print(f\"  Returning {len(downloaded_bytes)} raw bytes.\")\n",
        "            return downloaded_bytes\n",
        "        else:\n",
        "            print(f\"  ERROR: Invalid read_mode '{read_mode}'. Use 'r' or 'rb'.\")\n",
        "            return None\n",
        "\n",
        "    except HttpError as error:\n",
        "        print(f\"ERROR: An API error occurred: {error}\")\n",
        "        error_content = error.resp.reason if hasattr(error.resp, 'reason') else str(error)\n",
        "        print(f\"Error details: {error_content}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: An unexpected error occurred: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "    finally:\n",
        "        # --- 6. Clean up the temporary Colab file ---\n",
        "        if local_colab_temp_path and os.path.exists(local_colab_temp_path):\n",
        "            try:\n",
        "                os.remove(local_colab_temp_path)\n",
        "                print(f\"\\nCleanup: Temporary Colab file '{local_colab_temp_path}' deleted.\")\n",
        "            except Exception as e_clean:\n",
        "                print(f\"\\nWarning: Failed to delete temporary Colab file '{local_colab_temp_path}': {e_clean}\")\n",
        "        print(\"--- Process Finished ---\")"
      ],
      "metadata": {
        "id": "zhvD_zedsePs"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: Upload a text file, put it in 'MyColabTests/TextFiles' in Drive, read as text\n",
        "print(\"\\n=== EXAMPLE 1: UPLOAD AND READ TEXT FILE ===\")\n",
        "# The function will prompt you to select a file from your computer.\n",
        "# For testing, create a simple .txt file locally with some text.\n",
        "drive_path_for_txt = \"MyColabTests/TextFiles\"\n",
        "file_content = upload_local_to_drive_and_read(\n",
        "    drive_folder_path=drive_path_for_txt,\n",
        "    drive_filename_override=\"my_uploaded_document.txt\", # Optional: rename in Drive\n",
        "    overwrite_in_drive=True,\n",
        "    read_mode='r',\n",
        "    read_encoding='utf-8'\n",
        ")\n",
        "\n",
        "if file_content is not None:\n",
        "    print(\"\\n--- CONTENT READ FROM GOOGLE DRIVE ---\")\n",
        "    print(file_content)\n",
        "    print(\"------------------------------------\")\n",
        "else:\n",
        "    print(\"\\nFailed to get file content from Google Drive.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*40 + \"\\n\")\n",
        "\n",
        "# Example 2: Upload an image file, put it in 'MyColabTests/Images' in Drive, read as binary\n",
        "print(\"\\n=== EXAMPLE 2: UPLOAD AND READ BINARY (IMAGE) FILE ===\")\n",
        "# The function will prompt you again. Select a small image file (e.g., .png, .jpg).\n",
        "drive_path_for_img = \"MyColabTests/Images\"\n",
        "image_bytes = upload_local_to_drive_and_read(\n",
        "    drive_folder_path=drive_path_for_img,\n",
        "    # drive_filename_override=None, # Use original filename\n",
        "    overwrite_in_drive=True,\n",
        "    read_mode='rb' # Read as binary\n",
        ")\n",
        "\n",
        "if image_bytes is not None:\n",
        "    print(\"\\n--- BINARY CONTENT (FIRST 100 BYTES) READ FROM GOOGLE DRIVE ---\")\n",
        "    print(image_bytes[:100]) # Print only the first 100 bytes as it's binary\n",
        "    print(f\"Total bytes read: {len(image_bytes)}\")\n",
        "    print(\"----------------------------------------------------------------\")\n",
        "    # You could then, for example, display the image if you have PIL/matplotlib\n",
        "    # from PIL import Image\n",
        "    # import io\n",
        "    # try:\n",
        "    #     img = Image.open(io.BytesIO(image_bytes))\n",
        "    #     img.show() # This might not work directly in Colab, display(img) is better\n",
        "    #     from IPython.display import Image as IPImage, display\n",
        "    #     display(IPImage(data=image_bytes))\n",
        "    # except Exception as e_img:\n",
        "    #     print(f\"Could not display image: {e_img}\")\n",
        "else:\n",
        "    print(\"\\nFailed to get image byte content from Google Drive.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "h7IPtnDNynWD",
        "outputId": "09cc6516-44f0-4892-f723-a6adffa15ad1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EXAMPLE 1: UPLOAD AND READ TEXT FILE ===\n",
            "--- Initiating Local Upload to Drive and Read Process ---\n",
            "\n",
            "Step 1: Uploading file from your local computer to Colab...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7d2b686c-6218-4526-8509-ee847e067a8a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7d2b686c-6218-4526-8509-ee847e067a8a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Text File101promt.txt to Text File101promt.txt\n",
            "  Successfully uploaded 'Text File101promt.txt' (39 bytes) to Colab environment.\n",
            "  Saved uploaded content to temporary Colab file: '/tmp/tmpqtr1a79h_Text File101promt.txt'\n",
            "\n",
            "Step 2: Authenticating with Google Drive API...\n",
            "  Google Drive API authentication successful and service created.\n",
            "\n",
            "Step 3: Uploading file from Colab to Google Drive...\n",
            "  Target Drive path: 'MyColabTests/TextFiles'\n",
            "  Target Drive filename: 'my_uploaded_document.txt'\n",
            "Resolving Drive folder path: 'MyColabTests/TextFiles' from parent ID: 'root'\n",
            "Target Drive folder resolved to ID: 1AE4TfbvzS0AgXkOrZNt6__n27WT4j1uk\n",
            "  Found existing file 'my_uploaded_document.txt' in Drive (ID: 11WNAzmOtajm46rKG9c7I1lJNBQtMewSp).\n",
            "  Overwrite is True. Deleting existing file in Drive...\n",
            "  Successfully deleted existing Drive file ID: 11WNAzmOtajm46rKG9c7I1lJNBQtMewSp\n",
            "  Uploading 'my_uploaded_document.txt' (MIME: text/plain)...\n",
            "  SUCCESS: File uploaded to Google Drive.\n",
            "    Drive File Name: my_uploaded_document.txt\n",
            "    Drive File ID: 1Dj14EjltHp__Y0trQm93yQO1xKSfJOQo\n",
            "    Drive File Link: https://drive.google.com/file/d/1Dj14EjltHp__Y0trQm93yQO1xKSfJOQo/view?usp=drivesdk\n",
            "\n",
            "Step 4: Reading file content from Google Drive (ID: 1Dj14EjltHp__Y0trQm93yQO1xKSfJOQo)...\n",
            "  Download progress: 100%.\n",
            "  File content downloaded from Drive.\n",
            "  Successfully decoded 39 characters (encoding: utf-8).\n",
            "\n",
            "Cleanup: Temporary Colab file '/tmp/tmpqtr1a79h_Text File101promt.txt' deleted.\n",
            "--- Process Finished ---\n",
            "\n",
            "--- CONTENT READ FROM GOOGLE DRIVE ---\n",
            "tell me about the memrchant of vernice\n",
            "\n",
            "------------------------------------\n",
            "\n",
            "========================================\n",
            "\n",
            "\n",
            "=== EXAMPLE 2: UPLOAD AND READ BINARY (IMAGE) FILE ===\n",
            "--- Initiating Local Upload to Drive and Read Process ---\n",
            "\n",
            "Step 1: Uploading file from your local computer to Colab...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7fb24e25-44a3-40f7-aaaf-1954e61942b5\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7fb24e25-44a3-40f7-aaaf-1954e61942b5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Text File101promt.txt to Text File101promt (1).txt\n",
            "  Successfully uploaded 'Text File101promt (1).txt' (39 bytes) to Colab environment.\n",
            "  Saved uploaded content to temporary Colab file: '/tmp/tmpuw74s1v1_Text File101promt (1).txt'\n",
            "\n",
            "Step 2: Authenticating with Google Drive API...\n",
            "  Google Drive API authentication successful and service created.\n",
            "\n",
            "Step 3: Uploading file from Colab to Google Drive...\n",
            "  Target Drive path: 'MyColabTests/Images'\n",
            "  Target Drive filename: 'Text File101promt (1).txt'\n",
            "Resolving Drive folder path: 'MyColabTests/Images' from parent ID: 'root'\n",
            "Target Drive folder resolved to ID: 1yKEuHCa5KnuFNh0RvMfhLsGrOvHPRv0-\n",
            "  No existing file named 'Text File101promt (1).txt' found in the target Drive folder.\n",
            "  Uploading 'Text File101promt (1).txt' (MIME: text/plain)...\n",
            "  SUCCESS: File uploaded to Google Drive.\n",
            "    Drive File Name: Text File101promt (1).txt\n",
            "    Drive File ID: 1ITylzZJ6OILz_EOR9zjN6XlOSR2wZoVY\n",
            "    Drive File Link: https://drive.google.com/file/d/1ITylzZJ6OILz_EOR9zjN6XlOSR2wZoVY/view?usp=drivesdk\n",
            "\n",
            "Step 4: Reading file content from Google Drive (ID: 1ITylzZJ6OILz_EOR9zjN6XlOSR2wZoVY)...\n",
            "  Download progress: 100%.\n",
            "  File content downloaded from Drive.\n",
            "  Returning 39 raw bytes.\n",
            "\n",
            "Cleanup: Temporary Colab file '/tmp/tmpuw74s1v1_Text File101promt (1).txt' deleted.\n",
            "--- Process Finished ---\n",
            "\n",
            "--- BINARY CONTENT (FIRST 100 BYTES) READ FROM GOOGLE DRIVE ---\n",
            "b'tell me about the memrchant of vernice\\n'\n",
            "Total bytes read: 39\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pqb4DsxmkMOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# These variables store the names of different language models from Anthropic and OpenAI.\n",
        "# The \"SMART\" models (`claude-3-opus` and `gpt-4-turbo`) are more capable but slower,\n",
        "# while the \"FAST\" models (`claude-3-haiku` and `gpt-3.5-turbo`) are faster but less powerful.\n",
        "ANTHROPIC_SMART_MODEL_NAME = \"claude-3-opus-20240229\"\n",
        "ANTHROPIC_FAST_MODEL_NAME = \"claude-3-haiku-20240307\"\n",
        "OPENAI_SMART_MODEL_NAME = \"gpt-4.1\"\n",
        "OPENAI_FAST_MODEL_NAME = \"o3-2025-04-16\"\n",
        "\n",
        "# These variables point to two different markdown files containing prompt engineering guides.\n",
        "# `AMAN_PROMPT_GUIDE` refers to Aman Chadha's guide, while `LILIAN_PROMPT_GUIDE` refers to Lilian Weng's guide.\n",
        "AMAN_PROMPT_GUIDE = \"aman_prompt_engineering.md\"\n",
        "LILIAN_PROMPT_GUIDE = \"lilianweng_prompt_engineering.md\"\n",
        "\n",
        "# Here, the `MODEL_NAME` variable is set to use Anthropic's fast model (`claude-3-haiku`),\n",
        "# and the `PROMPT_GUIDE` variable selects Lilian Weng's prompt engineering guide.\n",
        "MODEL_NAME = OPENAI_FAST_MODEL_NAME\n",
        "PROMPT_GUIDE = file_content"
      ],
      "metadata": {
        "id": "2N4fU3h5kHb9"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weave.init(\"beginner-llm-prompting-course\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "L7fsjjZ9lM2s",
        "outputId": "78831ef3-fd04-44ef-caf8-b6fb86ba1d56"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please login to Weights & Biases (https://wandb.ai/) to continue:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=weave\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33medbertkwesi-ek\u001b[0m (\u001b[33medbertkwesi-ek-unilever\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logged in as Weights & Biases user: edbertkwesi-ek.\n",
            "View Weave data at https://wandb.ai/edbertkwesi-ek-unilever/beginner-llm-prompting-course/weave\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<weave.trace.weave_client.WeaveClient at 0x7db6ef6247d0>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "@weave.op()\n",
        "def get_completion(system_message: str, messages: list, model: str, max_tokens: int = 4096, temperature: float = 0, **kwargs) -> dict:\n",
        "    \"\"\"\n",
        "    Generates a completion using the specified model, taking into account the system message, conversation history, and additional arguments.\n",
        "\n",
        "    Parameters:\n",
        "        system_message (str): A message providing context or instructions for the model.\n",
        "        messages (list): A list of dictionaries representing the conversation history, where each dictionary has keys 'role' and 'content'.\n",
        "        model (str): The identifier of the model to use for generating completions.\n",
        "        max_tokens (int, optional): The maximum number of tokens to generate in the completion. Defaults to 4096.\n",
        "        temperature (float, optional): The sampling temperature to control the randomness of the generated text. Defaults to 0.\n",
        "        **kwargs: A dictionary of additional keyword arguments. Expected keys include 'system_message', 'model', 'max_tokens', and 'temperature'.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary representing the generated completion as JSON.\n",
        "    \"\"\"\n",
        "    # Adjust messages format based on the model type\n",
        "    if \"gpt\" in model.lower():\n",
        "        formatted_messages = [{\"role\": \"system\", \"content\": system_message}] + messages\n",
        "    else:\n",
        "        kwargs[\"system\"] = system_message  # For non-gpt models, use system_message directly in kwargs\n",
        "\n",
        "    # Common arguments for the completion function\n",
        "    completion_args = {\n",
        "        \"model\": model,\n",
        "        \"max_tokens\": max_tokens,\n",
        "        \"temperature\": temperature,\n",
        "        \"messages\": formatted_messages if \"gpt\" in model.lower() else messages,\n",
        "        **kwargs\n",
        "    }\n",
        "\n",
        "    # Generate and return the completion\n",
        "    response = completion(**completion_args)\n",
        "    return response.json()"
      ],
      "metadata": {
        "id": "v5C5FOumltvm"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@weave.op()\n",
        "def prompt_llm(question: str, **kwargs) -> str:\n",
        "    \"\"\"\n",
        "    Sends a question to the language model and returns its response.\n",
        "\n",
        "    This function prepares a message with the user's question, handles additional\n",
        "    arguments for the language model, and invokes the get_completion function to\n",
        "    obtain a response. The response's content is then returned.\n",
        "\n",
        "    Parameters:\n",
        "        question (str): The question intended for the language model.\n",
        "        #**kwargs: A dictionary of additional keyword arguments. Expected keys include 'system_message', 'model', 'max_tokens', and 'temperature'.\n",
        "\n",
        "    Returns:\n",
        "        str: The language model's response to the question.\n",
        "    \"\"\"\n",
        "    # Prepare the user's question for the language model\n",
        "    messages = [{\"role\": \"user\", \"content\": question}]\n",
        "\n",
        "    # Extract additional parameters, applying defaults if necessary\n",
        "    system_message = kwargs.pop('system_message', \"\")\n",
        "    model = kwargs.pop('model', MODEL_NAME)\n",
        "    max_tokens = kwargs.pop('max_tokens', 4000)\n",
        "    temperature = kwargs.pop('temperature', 0)\n",
        "\n",
        "    # Compile arguments for the completion request\n",
        "    completion_args = {\n",
        "        \"system_message\": system_message,\n",
        "        \"messages\": messages,\n",
        "        \"model\": model,\n",
        "        \"max_tokens\": max_tokens,\n",
        "        \"temperature\": temperature\n",
        "    }\n",
        "    completion_args.update(kwargs)  # Properly include any other additional arguments\n",
        "\n",
        "    # Request a completion from the language model\n",
        "    response = get_completion(**completion_args)\n",
        "@weave.op()\n",
        "def prompt_llm(question: str, **kwargs) -> str:\n",
        "    \"\"\"\n",
        "    Sends a question to the language model and returns its response.\n",
        "\n",
        "    This function prepares a message with the user's question, handles additional\n",
        "    arguments for the language model, and invokes the get_completion function to\n",
        "    obtain a response. The response's content is then returned.\n",
        "\n",
        "    Parameters:\n",
        "        question (str): The question intended for the language model.\n",
        "        **kwargs: A dictionary of additional keyword arguments. Expected keys include 'system_message', 'model', 'max_tokens', and 'temperature'.\n",
        "\n",
        "    Returns:\n",
        "        str: The language model's response to the question.\n",
        "    \"\"\"\n",
        "    # Prepare the user's question for the language model\n",
        "    messages = [{\"role\": \"user\", \"content\": question}]\n",
        "\n",
        "    # Extract additional parameters, applying defaults if necessary\n",
        "    system_message = kwargs.pop('system_message', \"\")\n",
        "    model = kwargs.pop('model', MODEL_NAME)\n",
        "    max_tokens = kwargs.pop('max_tokens', 4000)\n",
        "    temperature = kwargs.pop('temperature', 1)\n",
        "\n",
        "    # Compile arguments for the completion request\n",
        "    completion_args = {\n",
        "        \"system_message\": system_message,\n",
        "        \"messages\": messages,\n",
        "        \"model\": model,\n",
        "        \"max_tokens\": max_tokens,\n",
        "        \"temperature\": temperature\n",
        "    }\n",
        "    completion_args.update(kwargs)  # Properly include any other additional arguments\n",
        "\n",
        "    # Request a completion from the language model\n",
        "    response = get_completion(**completion_args)\n",
        "\n",
        "    # Extract and return the content of the model's response\n",
        "    return response[\"choices\"][0][\"message\"][\"content\"]"
      ],
      "metadata": {
        "id": "C5EYmoKjmbos"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_prompt_response = prompt_llm(\n",
        "    \"Explain the latest prompting techniques and provide an example of each\"\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        },
        "id": "V6FZhLBbnDtO",
        "outputId": "d44cadb2-a045-4641-e5ec-d067bab3c5a6"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotFoundError",
          "evalue": "litellm.NotFoundError: OpenAIException - Your organization must be verified to use the model `o3-2025-04-16`. Please go to: https://platform.openai.com/settings/organization/general and click on Verify Organization. If you just verified, it can take up to 15 minutes for access to propagate.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/litellm/llms/openai/openai.py\u001b[0m in \u001b[0;36mcompletion\u001b[0;34m(self, model_response, timeout, optional_params, litellm_params, logging_obj, model, messages, print_verbose, api_key, api_base, api_version, dynamic_params, azure_ad_token, acompletion, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params)\u001b[0m\n\u001b[1;32m    724\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 725\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOpenAIError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/litellm/llms/openai/openai.py\u001b[0m in \u001b[0;36mcompletion\u001b[0;34m(self, model_response, timeout, optional_params, litellm_params, logging_obj, model, messages, print_verbose, api_key, api_base, api_version, dynamic_params, azure_ad_token, acompletion, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params)\u001b[0m\n\u001b[1;32m    652\u001b[0m                             \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m                         \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_sync_openai_chat_completion_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m                             \u001b[0mopenai_client\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopenai_client\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\u001b[0m in \u001b[0;36msync_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/litellm/llms/openai/openai.py\u001b[0m in \u001b[0;36mmake_sync_openai_chat_completion_request\u001b[0;34m(self, openai_client, data, timeout, logging_obj)\u001b[0m\n\u001b[1;32m    470\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/litellm/llms/openai/openai.py\u001b[0m in \u001b[0;36mmake_sync_openai_chat_completion_request\u001b[0;34m(self, openai_client, data, timeout, logging_obj)\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             raw_response = openai_client.chat.completions.with_raw_response.create(\n\u001b[0m\u001b[1;32m    454\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_legacy_response.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLegacyAPIResponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/weave/trace/op.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    679\u001b[0m                 \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m                     res, _ = _call_sync_func(\n\u001b[0m\u001b[1;32m    681\u001b[0m                         \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__should_raise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/weave/trace/op.py\u001b[0m in \u001b[0;36m_call_sync_func\u001b[0;34m(op, __weave, __should_raise, __require_explicit_finish, *args, **kwargs)\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/weave/integrations/openai/openai_sdk.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stream_options\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"include_usage\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    930\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1275\u001b[0m         )\n\u001b[0;32m-> 1276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    950\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1056\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1057\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1058\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: Error code: 404 - {'error': {'message': 'Your organization must be verified to use the model `o3-2025-04-16`. Please go to: https://platform.openai.com/settings/organization/general and click on Verify Organization. If you just verified, it can take up to 15 minutes for access to propagate.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/litellm/main.py\u001b[0m in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[0m\n\u001b[1;32m   1832\u001b[0m                 )\n\u001b[0;32m-> 1833\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/litellm/main.py\u001b[0m in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[0m\n\u001b[1;32m   1805\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1806\u001b[0;31m                 response = openai_chat_completions.completion(\n\u001b[0m\u001b[1;32m   1807\u001b[0m                     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/litellm/llms/openai/openai.py\u001b[0m in \u001b[0;36mcompletion\u001b[0;34m(self, model_response, timeout, optional_params, litellm_params, logging_obj, model, messages, print_verbose, api_key, api_base, api_version, dynamic_params, azure_ad_token, acompletion, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params)\u001b[0m\n\u001b[1;32m    735\u001b[0m                 \u001b[0merror_headers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"headers\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m             raise OpenAIError(\n\u001b[0m\u001b[1;32m    737\u001b[0m                 \u001b[0mstatus_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOpenAIError\u001b[0m: Error code: 404 - {'error': {'message': 'Your organization must be verified to use the model `o3-2025-04-16`. Please go to: https://platform.openai.com/settings/organization/general and click on Verify Organization. If you just verified, it can take up to 15 minutes for access to propagate.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-4f1a34c5fc51>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m raw_prompt_response = prompt_llm(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"Explain the latest prompting techniques and provide an example of each\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m )\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/weave/trace/op.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    678\u001b[0m                 \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m                 \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m                     res, _ = _call_sync_func(\n\u001b[0m\u001b[1;32m    681\u001b[0m                         \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__should_raise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/weave/trace/op.py\u001b[0m in \u001b[0;36m_call_sync_func\u001b[0;34m(op, __weave, __should_raise, __require_explicit_finish, *args, **kwargs)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-72-24acbdad4af8>\u001b[0m in \u001b[0;36mprompt_llm\u001b[0;34m(question, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;31m# Request a completion from the language model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mcompletion_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;31m# Extract and return the content of the model's response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/weave/trace/op.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    678\u001b[0m                 \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m                 \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m                     res, _ = _call_sync_func(\n\u001b[0m\u001b[1;32m    681\u001b[0m                         \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__should_raise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/weave/trace/op.py\u001b[0m in \u001b[0;36m_call_sync_func\u001b[0;34m(op, __weave, __should_raise, __require_explicit_finish, *args, **kwargs)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-56-85c32517ef71>\u001b[0m in \u001b[0;36mget_completion\u001b[0;34m(system_message, messages, model, max_tokens, temperature, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# Generate and return the completion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompletion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mcompletion_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/weave/trace/op.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    678\u001b[0m                 \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m                 \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m                     res, _ = _call_sync_func(\n\u001b[0m\u001b[1;32m    681\u001b[0m                         \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__should_raise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/weave/trace/op.py\u001b[0m in \u001b[0;36m_call_sync_func\u001b[0;34m(op, __weave, __should_raise, __require_explicit_finish, *args, **kwargs)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/litellm/utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1277\u001b[0m                     \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback_exception\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m                 )  # DO NOT MAKE THREADED - router retry fallback relies on this!\n\u001b[0;32m-> 1279\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1281\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/litellm/utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1155\u001b[0m                     \u001b[0mprint_verbose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error while checking max token limit: {str(e)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m             \u001b[0;31m# MODEL CALL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1157\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1158\u001b[0m             \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"stream\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stream\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/litellm/main.py\u001b[0m in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[0m\n\u001b[1;32m   3218\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3219\u001b[0m         \u001b[0;31m## Map to OpenAI Exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3220\u001b[0;31m         raise exception_type(\n\u001b[0m\u001b[1;32m   3221\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3222\u001b[0m             \u001b[0mcustom_llm_provider\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_llm_provider\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\u001b[0m in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[1;32m   2230\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexception_mapping_worked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2231\u001b[0m             \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"litellm_response_headers\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlitellm_response_headers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2232\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2233\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2234\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0merror_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlitellm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLITELLM_EXCEPTION_TYPES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\u001b[0m in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m                 ):\n\u001b[1;32m    303\u001b[0m                     \u001b[0mexception_mapping_worked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m                     raise NotFoundError(\n\u001b[0m\u001b[1;32m    305\u001b[0m                         \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"{exception_provider} - {message}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                         \u001b[0mllm_provider\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_llm_provider\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: litellm.NotFoundError: OpenAIException - Your organization must be verified to use the model `o3-2025-04-16`. Please go to: https://platform.openai.com/settings/organization/general and click on Verify Organization. If you just verified, it can take up to 15 minutes for access to propagate."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "render(raw_prompt_response)"
      ],
      "metadata": {
        "id": "edqleqYbnLzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_prompt_response = prompt_llm(\n",
        "    context + \"\\n\\nExplain the latest prompting techniques and provide an example of each\"\n",
        ")"
      ],
      "metadata": {
        "id": "jOdAeh1WqsVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@weave.op()\n",
        "def get_completion(system_message: str, messages: list, model: str, max_tokens: int = 4096, temperature: float = 0, **kwargs) -> dict:\n",
        "    \"\"\"\n",
        "    Generates a completion using the specified model, taking into account the system message, conversation history, and additional arguments.\n",
        "\n",
        "    Parameters:\n",
        "        system_message (str): A message providing context or instructions for the model.\n",
        "        messages (list): A list of dictionaries representing the conversation history, where each dictionary has keys 'role' and 'content'.\n",
        "        model (str): The identifier of the model to use for generating completions.\n",
        "        max_tokens (int, optional): The maximum number of tokens to generate in the completion. Defaults to 4096.\n",
        "        temperature (float, optional): The sampling temperature to control the randomness of the generated text. Defaults to 0.\n",
        "        **kwargs: A dictionary of additional keyword arguments. Expected keys include 'system_message', 'model', 'max_tokens', and 'temperature'.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary representing the generated completion as JSON.\n",
        "    \"\"\"\n",
        "    # Adjust messages format based on the model type\n",
        "    if \"gpt\" in model.lower():\n",
        "        formatted_messages = [{\"role\": \"system\", \"content\": system_message}] + messages\n",
        "    else:\n",
        "        kwargs[\"system\"] = system_message  # For non-gpt models, use system_message directly in kwargs\n",
        "\n",
        "    # Common arguments for the completion function\n",
        "    completion_args = {\n",
        "        \"model\": model,\n",
        "        \"max_tokens\": max_tokens,\n",
        "        \"temperature\": temperature,\n",
        "        \"messages\": formatted_messages if \"gpt\" in model.lower() else messages,\n",
        "        **kwargs\n",
        "    }\n",
        "\n",
        "    # Generate and return the completion\n",
        "    # Use litellm.completion instead of client.completion.create\n",
        "    response = litellm.completion(**completion_args)\n",
        "    return response.json() if hasattr(response, 'json') else response # Ensure response can be converted to JSON"
      ],
      "metadata": {
        "id": "FlXcph3GFe4x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}